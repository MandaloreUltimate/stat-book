\section{Проверка гипотез. Лемма Неймана"--~Пирсона}

\begin{defn}
    \textit{Статистическая гипотеза $H$}~--- любое предположение о распределении наблюдаемой случайной величины.
    
    Гипотеза называется \textit{простой}, если в ней явно задаётся одно (не параметризованное) распределение. 
    Например, $H\colon X_i \sim \Normal_{0, 1}$. 
    В противном случае гипотеза называется \textit{сложной}.

    Как правило, рассматривается сразу две взаимоисключающие гипотезы.
    Одна из них называется \textit{основной} и обозначается $H_0$, а другая~--- \textit{альтернативной} и обозначается $H_1$.
\end{defn}

Гипотезы могут быть самыми разнообразными.
Приведём несколько примеров.

\begin{enumerate}
    \item 
        Гипотезы о виде распределения: \\
        Пусть производится $n$ независимых наблюдений над некоторой случайной величиной $\xi$ с неизвестной функцией распределения $F_{\xi}(x)$.
        Основная гипотеза~--- $H_0 \colon F_{\xi}(x) = F(x)$ или $H_0 \colon F_{\xi}(x) \in \SigAlg$, где $\SigAlg$~--- 
        некоторое подмножество в множестве всех распределений (как правило, $\SigAlg$ задаётся параметрически).
        %В этом случае гипотеза называется \textit{простой}, если она состоит из \textit{одного} значения параметра, и \textit{сложной} иначе;
    \item 
        Гипотезы о проверке однородности выборки: \\
        Произведено $k$ серий независимых наблюдений и получено $k$ выборок $\SampleX_1, \ldots, \SampleX_k$.
        Основная гипотеза состоит в том, что эти выборки извлечены из одного распределения, т.е.
        $H_0 \colon F_1(x) \equiv \ldots \equiv F_k(x)$, где $F_i$~--- функция распределения элементов $i$-й выборки.
    \item 
        Гипотеза независимости: \\
        Наблюдается двумерная случайная величина $\xi = (\xi_1, \xi_2)$ с неизвестной функцией распределения $F(x, y)$.
        Получена выборка $(X_1, Y_1), \ldots, (X_n, Y_n)$.
        Основная гипотеза заключается в том, что $\xi_1$ и $\xi_2$ независимы, 
        то есть $H_0\colon F(x, y) = F_{\xi_1}(x) F_{\xi_2}(y)$, где $F_{\xi_1}, F_{\xi_2}$~--- некоторые одномерные функции распределения.
        В общем случае можно рассматривать $k$-мерную случайную величину и проверять гипотезу независимости её компонент.
        
        %по выборке $(X_1, Y_1), \ldots, (X_n, Y_n)$ из $n$ независимых наблюдений пары случайных величин проверяется 
        %гипотеза $H_{0}=\{X_{i} \text { и } Y_{i} \text { независимы }\}$ при альтернативе $H_{1}=\{ \text { предположение неверно } \}$. 
        %Обе гипотезы являются сложными;
    \item 
        Гипотеза случайности: \\
        Результат эксперимента описывается $n$-мерной случайной величиной $X = \Sample$ с неизвестной функцией распределения $F_{X}(x_1, \ldots, x_n)$.
        Можно ли рассматривать $X$ как выборку из распределения некоторой случайной величины $\xi$ (т.е. являются ли компоненты $X_i$ независимыми и одинаково распределёнными)?
        В данном случае проверяется гипотеза случайности $H_0\colon F_{X}(x_1, \ldots, x_n) = F_{\xi}(x_1) \cdot \ldots \cdot F_{\xi}(x_n)$.

\end{enumerate}

\begin{rmrk}
    Может показаться, что в общем случае задача имеет такой вид:
    "<Дана выборка $\SampleX$. Верна ли гипотеза $H_0$?">
    Но корректнее говорить "<Согласуются ли наблюдаемые данные с высказанной гипотезой?">.

    В дальнейшем мы увидим, что процесс проверки гипотезы скорее напоминает доказательство неверности $H_0$.
    Мы оцениваем, насколько вероятна имеющаяся реализация выборки в предположении, что $H_0$ истинна.
    Если наблюдаемые значения достаточно маловероятны, то мы считаем, что гипотеза $H_0$ неправдоподобна и отвергаем её.
    В противном случае мы лишь можем сказать, что данные не противоречат высказанной гипотезе.
    
    Приведём модельный пример: пусть есть выборка из одного элемента $\SampleX= X_1$ и проверяется простая гипотеза $H_0\colon X_1 \sim U[-2, 2]$.
    Если наблюдаемая реализация выборки $x_1 = 3$, то наша гипотеза очевидно неверна, и мы можем спокойно её отвергнуть.
    Но если $x_1 = 1$, то мы не можем утверждать, что $H_0$ справедлива: 
    мы могли получить $x_1 = 1$ и из стандартного нормального распределения, и из пуассоновского, или из отрицательного биномиального.
\end{rmrk}

\medskip
В приведённом выше замечании мы уже сформулировали одну гипотезу, и, по сути, даже проверили её, 
интуитивно построив алгоритм принятия решения: если $x_1 \notin [-2, 2]$, то $H_0$ отвергается, иначе оснований считать её неверной нет.
Для того, чтобы проверять более сложные гипотезы, нам понадобится строить похожие алгоритмы, или критерии~--- правила, согласно которым \textit{по выборке} делается заключение о верности гипотезы.

\begin{defn}
    Критерий~--- это статистика $\varphi(\SampleX)$ (т.е. измеримая функция от выборки) со значениями из $[0, 1]$. 
    Трактуется как "<вероятность"> отвергнуть~$H_0$.
    
    Если $\varphi(\SampleX) \overset{\text{п.н.}}{\in} \{0, 1\}$, 
    то критерий называется \textit{нерандомизированным}, иначе~--- \textit{рандомизированным}.
\end{defn}

\begin{rmrk}
    С нерандомизированным критерием всё понятно: на каждой реализации выборки он даёт однозначный ответ, 
    принять ($\varphi(\boldsymbol{x}) = 0$) или отвергнуть ($\varphi(\boldsymbol{x}) = 1$) гипотезу.
    Но что делать, если $\varphi(\boldsymbol{x}) \in (0, 1)$?
    Тогда мы отвергаем гипотезу $H_0$ с вероятностью $\varphi(\boldsymbol{x})$.
    По сути, мы принимаем решение, подбрасывая (асимметричную) монетку.
    Понятно, что нерандомизированные критерии лучше, но, к сожалению, некоторые задачи не получается решить, отвечая лишь "<да"> и "<нет">.\\
\end{rmrk}

Каким образом строится критерий?

Выборка $\SampleX= \Sample$ объёма $n$~--- точка в пространстве $\Real^{n}$. 
Выделим такое множество $S \subset \Real^{n}$, что ${\MyPrTh \Bigl(\SampleX\in S \, | \, \{H_0 \text{ верна}\}\Bigr) \leqslant \alpha},$
где $\alpha \in (0, 1)$~--- некоторое наперёд заданное число.
Это множество называется \textit{критической областью} для гипотезы $H_0$. 
В этом случае критерий можно сформулировать следующим образом:
\begin{compactlist}
    \item $\SampleX\in S \implies$ отвергаем $H_0$;
    \item $\SampleX\notin S \implies$ нет оснований отвергать $H_0$, считаем её верной.
\end{compactlist}
Или $\varphi(\SampleX) = \Ind(\SampleX\in S)$ \textit{(нерандомизированный!)}.

По смыслу критическая область~--- это множество таких значений выборки, которые маловероятны при условии истинности $H_0$.
Поэтому при попадании выборки в критическую область основная гипотеза отвергается, как противоречащая статистическим данным.

\medskip
Конечно, если мы построили критерий и выбрали какую-то гипотезу, это не значит, что она стопроцентно верна.
Может оказаться, что мы отвергнули верную, или приняли ложную гипотезу.
Ошибки при проверке гипотез делятся на два типа.

\begin{defn}
    Говорят, что произошла \textit{ошибка 1-го рода (false positive)}, если критерий отверг верную гипотезу $H_0$. 
    Вероятность ошибки 1-го рода: 
    \begin{equation*}
        \alpha(S)=\MyPrTh\left(\SampleX\in S | \, H_{0}\right) = \MyPr{0}\left(\SampleX\in S\right)
    \end{equation*}
    Аналогично вероятность \textit{ошибки 2-го рода (false negative)}:
    \begin{equation*}
        \beta(S)=\MyPrTh\left(\SampleX\notin S | \, H_{1}\right)=\MyPr{1}\left(\SampleX\notin S\right)
    \end{equation*}
\end{defn}

\begin{rmrk}
    Вероятность ошибки 1-го рода также называется \textit{уровнем значимости критерия}.
    Выше мы говорили о том, что критическая область~--- это множество тех реализаций выборки, которые маловероятны при истинности основной гипотезы.
    При помощи $\alpha = \alpha(S)$ мы как раз и определяем, насколько маловероятно это событие.
    Естественным образом получается, что вероятность отвергнуть верную основную гипотезу равна $\alpha$.
\end{rmrk}

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline \multirow{2}{*} { Истинная гипотеза } & \multicolumn{2}{|c|} { Результат принятия решения } \\
        \cline {2-3} & $H_{0}$ принята & $H_{0}$ отклонена \\
        \hline $H_{0}$ & $1-\alpha$ & $\alpha$ \\
        \hline $H_{1}$ & $\beta$ & $1-\beta$ \\
        \hline
    \end{tabular}
\end{center}

Если $1 - \beta(S) < \alpha(S)$, то попасть в $S$ при условии истинности гипотезы $H_1$ труднее, чем при условии истинности гипотезы $H_0$, 
и тогда $S$~--- критическая область скорее для $H_1$. 
Хотелось бы, чтобы неравенство имело вид ${1 - \beta(S) \geqslant \alpha(S)}$.

\begin{defn}
    Критерий называется \textit{несмещённым}, если выполняется условие
    \begin{equation*}
        1 - \beta(S) \geqslant \alpha(S).
    \end{equation*}
\end{defn}

В общем случае не получается сделать сумму вероятностей ошибок обоих родов сколь угодно малой, 
так как задачи минимизации каждой из ошибок, как правило, противоречат друг другу.
Например, если наш критерий всегда будет отвергать основную гипотезу~--- $\varphi(\SampleX) \equiv 1$, то вероятность ошибки второго рода равна нулю.
И наоборот, всегда принимая основную гипотезу~--- $\varphi(\SampleX) \equiv 0$, мы никогда не совершим ошибку первого рода.

Конечно, можно привести пример, где есть идеальный критерий.
Пусть мы знаем, что наблюдаемая случайная величина распределена равномерно либо на отрезке $[0, 1]$ (основная гипотеза), либо на $[2, 3]$ (альтернатива).
Тогда критической областью будет $S = [2, 3]$, и критерий $\varphi(\SampleX) = \Ind(x_1 \in [2,3])$ будет иметь нулевые вероятности ошибок обоих родов.
Причиной тому то, что области значений исследуемых случайных величин не пересекаются. Однако такой пример весьма далёк от реальных ситуаций.

\medskip
Если мы не можем минимизировать и то и другое, логично зафиксировать один из параметров и оптимизировать оставшийся.
Обычно фиксируют уровень значимости, т.е. вероятность ошибки первого рода $\alpha$.

Рассмотрим так называемые \textit{параметрические гипотезы}.
Считаем, что выборка $\SampleX$ взята из некоторого параметрического семейства 
$\SigAlg = {\{F_{\theta}(x), \: \theta \in \Theta \subseteq \Real^r\}}, \; \theta = (\theta_1, \ldots, \theta_r)$. \\
\begin{tabular}{rl}
    Основная гипотеза~--- & $H_0\colon \theta \in \Theta_0, \: \Theta_0 \subset \Theta.$ \\
    Альтернатива~---   & $H_1\colon \theta \in \Theta_1, \: \Theta_1 = \Theta \setminus \Theta_0.$
\end{tabular}

\begin{defn}
    \textit{Мощность критерия}:
    \begin{equation*}
        W(\theta, \varphi) = \ExpTh \varphi(\SampleX) = \int\limits_{x \in \Real^n} \varphi(x) L(\mathbf{x}, \theta) \, dx.
        \footnote{Напоминаем, что при фиксированном $\theta$ функция правдоподобия является вероятностной мерой на выборочном пространстве.}
    \end{equation*}
\end{defn}

\begin{rmrk}
    Заметим, что если критерий $\varphi(\SampleX)$ нерандомизированный 
    (т.е. принимает только значения 0 и 1, однозначно решая, отвергнуть или принять основную гипотезу), 
    то
    \begin{equation*}
        W(\theta, \varphi) = \ExpTh \varphi(\SampleX) = 1 \cdot \MyPrTh (\SampleX\in S) + 0 \cdot \MyPrTh (\SampleX\notin S) = \MyPrTh(\SampleX\in S).
    \end{equation*} 
    Если $\theta \in \Theta_0$, то $\MyPrTh(\SampleX\in S | H_0) = \alpha(S)$. \\
    Если $\theta \in \Theta_1$, то $\MyPrTh(\SampleX\in S | H_1) = 1 - \MyPrTh(\SampleX\notin S | H_1) = 1 - \beta(S)$.
    Т.е. ошибки и первого, и второго рода можно выразить через функцию мощности.
\end{rmrk}

Разберёмся сначала с простыми гипотезами: $H_0\colon \theta = \theta_0, H_1\colon \theta = \theta_1$. 
Зададим $\alpha_0$ и будем иметь дело только с такими критериями, где $\alpha_{0} \geqslant \alpha(S)$ 
(т.е. вероятность ошибки первого рода не превосходит величины $\alpha_0$) и будем решать задачу $\beta(S) \to \min\limits_{S}$, 
что равносильно максимизации мощности при $\theta \in \Theta_1$ (в нашем случае $\Theta_1 = \{\theta_1\}$, и мы максимизируем $W(\theta_1, \varphi)$).

Получаем две эквивалентные задачи определения критической области $S$:
\begin{equation*}
    \begin{cases}
        \alpha(S) \leqslant \alpha_{0} \\
        \beta(S) \to \min\limits_{S}
    \end{cases}
    \iff~
    \begin{cases}
        W(\theta_0, \varphi) = \MyPr{\theta_0} (\SampleX\in S | H_0) \leqslant \alpha_0  \\
        W(\theta_1, \varphi) = \MyPr{\theta_1} (\SampleX\in S | H_1) \to \max\limits_{S} 
    \end{cases}
\end{equation*}

К сожалению, не всегда удаётся решить такую задачу, используя только нерандомизированные критерии $\varphi(\SampleX) = \Ind(\SampleX\in S)$.
Тяжело вздохнём и обратимся к рандомизированным.

Пусть дана выборка $\SampleX= \Sample$, относительно которой выдвинуто две простых параметрических гипотезы: 
$H_0\colon \theta = \theta_0$ и $H_1\colon \theta = \theta_1$.
Без ограничения общности будем предполагать, что существует плотность $f_{0}(x)$ для функции распределения $F_{0}(x) = F_{\theta_0}(x)$, соответствующей гипотезе $\theta = \theta_0$, 
и существует плотность $f_{1}(x)$ для функции распределения $F_{1}(x) = F_{\theta_1}(x)$.
В дискретном случае все результаты аналогичны.

Если верна гипотеза $H_1$, то функция правдоподобия выборки $X$ имеет вид:
\begin{equation*}
    L(\SampleX, \theta_1) =  L_{1}\left(\SampleX\right) = \prod_{i=1}^{n} f_{1}\left(X_{i}\right).
\end{equation*}

В противном случае 
\begin{equation*}
    L(\SampleX, \theta_0) = L_{0}\left(\SampleX\right) = \prod_{i=1}^{n} f_{0}\left(X_{i}\right).
\end{equation*}

\begin{rmrk}
    Вероятность ошибки первого и второго рода для рандомизированного критерия будем обозначать
    $\alpha(\varphi)$ и $\beta(\varphi)$ соответственно.
\end{rmrk}

Посчитаем вероятность ошибок, используя то, что при каждом $x \in \Real^n$ значение $\varphi(x)$~--- это вероятность отвергнуть $H_0$.
\begin{gather*}
    \MyPrTh\bigl(H_0 \text{ отвергнута} | H_0 \text{ верна}\bigr) = 
    \MyPr{0} \left(\overline{H}_{0}\right) = 
    \int\limits_{x \in \Real^{n}} \varphi(x) L_{0}(x)\, dx = \alpha(\varphi) \\
    \MyPrTh\bigl(H_0 \text{ принята} | H_0 \text{ неверна}\bigr) = 
    \MyPr{1} \left(H_{0}\right) = 
    \int\limits_{x \in \Real^{n}}(1-\varphi(x)) L_{1}(x)\, dx = \beta(\varphi) \\
\end{gather*}
Т.е. 
\begin{align*}
    W(\theta_0, \varphi) = &~\alpha(\varphi) \\
    W(\theta_1, \varphi) = &~1 - \beta(\varphi)
\end{align*}

Тогда задача построения статистического критерия %сводится к нахождению критической функции $\varphi(x)$ и 
будет формулироваться следующим образом:
\begin{equation*}
    \begin{cases}
        \alpha(\varphi) \leqslant \alpha_{0} \\
        \beta(\varphi) \to \min\limits_{\varphi}
    \end{cases}
    \iff
    \begin{cases}
        W(\theta_0, \varphi) = 
        \Exp_{\theta_0} \varphi(\SampleX) = 
        \Exp_{0} \varphi(\SampleX) \leqslant \alpha_{0} \\
        W(\theta_1, \varphi) = 
        \Exp_{\theta_1} \varphi(\SampleX) = 
        \Exp_{1} \varphi(\SampleX)\to \max\limits_{\varphi}
    \end{cases}
\end{equation*}
Таким образом, задача заключается в том, чтобы найти наиболее мощный критерий, 
когда вероятность ошибки первого рода не превосходит некоторого заданного порогового значения. 
Решение сформулированных задач даётся леммой Неймана"--~Пирсона.

\begin{namedthm}[Лемма Неймана"--~Пирсона]
    Пусть $\alpha_{0} \in(0, 1)$.
    Введём отношение функций правдоподобия $l(\SampleX) = \cfrac{L_{1}(\SampleX)}{L_{0}(\SampleX)}$.
    Тогда при фиксированной вероятности ошибки первого рода $\alpha_{0}$ наиболее мощный критерий (сокращенно НМК) имеет вид
    \begin{equation*}
        \varphi^{*}(x) = \begin{cases}
            1, & \text { если } l(\SampleX) > C \\
            \varepsilon, & \text { если } l(\SampleX) = C \\
            0, & \text { если } l(\SampleX) < C
        \end{cases}
    \end{equation*}
    где константы $C$ и $\varepsilon$ являются решениями уравнения "<вероятность ошибки первого рода для этого критерия равна $\alpha_0$">: 
    $\alpha\left(\varphi^{*}\right)=\alpha_{0}$.
\end{namedthm}

\begin{rmrk}
    Если для некоторой реализации выборки получится так, что $L_0(\boldsymbol{x}) = 0, L_1(\boldsymbol{x}) > 0$,
    то просто будем считать, что $l(\boldsymbol{x}) = +\infty > C$, и гипотезу $H_0$ надо отвергнуть.
\end{rmrk}

\begin{proof}
    \begin{enumerate}
        \item Покажем, что константы $C$ и $\varepsilon$ могут быть найдены из уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$. 
        Заметим, что
        
        \begin{equation*}
            \begin{aligned} \alpha(\varphi^{*})
            = P_{0}\Bigl(l(\SampleX) > C \Bigr)
            + \varepsilon P_{0} \Bigl(l(\SampleX) = C \Bigr) 
            \end{aligned}
        \end{equation*}

    %Если предположить, что $L_{0}(\SampleX)=0$, то

    %\begin{equation*}
    %    P_{0}\left\{L_{0}(\SampleX) = 0\right\} = \int\limits_{\left\{x: L_{0}(x)=0\right\}} L_{0}(x) \mu(d x)=0
    %\end{equation*}

    %и, следовательно, вышеприведённое равенство корректно. 
    %Поэтому рассмотрим случайную величину $\eta(\SampleX) = \cfrac{L_{1}(\SampleX)}{L_{0}(\SampleX)}$
    Отношение правдоподобий $l(\SampleX)$~--- случайная величина.
    Для удобства обозначим $\eta = l(\SampleX)$.
    Пусть $F_{\eta, H_0}(x)$~--- функция распределения этой величины в предположении, что $H_0$ верна.
 
    Тогда
    \begin{equation*}
        \alpha \left(\varphi^{*}\right) = 
        1-F_{\eta, H_0}(C) + \varepsilon\Bigl(F_{\eta, H_0}(C) - F_{\eta, H_0}(C - 0)\Bigr)
    \end{equation*}

    Пусть $g(C) = 1 - F_{\eta, H_0}(C)$, константу $C_{\alpha_{0}}$ можно выбрать так, чтобы было выполнено неравенство:
    \begin{equation*}
        g(C_{\alpha_{0}}) \leqslant \alpha_{0} \leqslant g(C_{\alpha_{0}} - 0)
    \end{equation*}
    Тогда
    \begin{equation*}
        \varepsilon_{\alpha_{0}} = 
        \begin{cases}
            0, & \text{ если }  g(C_{\alpha_{0}}) = g(C_{\alpha_{0}} - 0) \\
            \cfrac{\alpha_{0} - g\left(C_{\alpha_{0}}\right)}{g(C_{\alpha_{0}}-0)-g(C_{\alpha_{0}})} \in [0, 1], & \text{ если } g(C_{\alpha_{0}}) < g(C_{\alpha_{0}}-0)
        \end{cases}
    \end{equation*}

    В обоих случаях выполнено равенство:
    \begin{equation*}
        \alpha_{0} = 
        g(C_{\alpha_{0}}) + \varepsilon_{\alpha_{0}}\Bigl(g(C_{\alpha_{0}} - 0) - g(C_{\alpha_{0}})\Bigr) =
        \alpha\left(\varphi^{*}\right)
    \end{equation*}

    \item Докажем, что $\varphi^{*}(x)$~--- наиболее мощный критерий.

    Выберем любой другой критерий $\tilde{\varphi}(x)$ такой, что $\alpha(\tilde{\varphi}) \leqslant \alpha_{0}$, 
    и сравним ее с критерием $\varphi^{*}(x)$. 
    Заметим, что для любого $x$ справедливо неравенство:
    \begin{equation*}
        \bigl(\varphi^{*}(x)-\tilde{\varphi}(x)\bigr) \Bigl(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\Bigr) \geqslant 0
    \end{equation*}

    Тогда
    \begin{equation*}
        \int\limits_{\Real^{n}} \bigl(\varphi^{*}(x)-\tilde{\varphi}(x)\bigr) \Bigl(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\Bigr) \, dx \geqslant 0
    \end{equation*}

    Раскроем скобки и преобразуем:
    \begin{multline*}
        \int\limits_{\Real^{n}} \varphi^{*}(x) L_{1}(x) \, dx - \int\limits_{\Real^{n}} \tilde{\varphi}(x) L_{1}(x) \,dx \geqslant \\
        \geqslant C_{\alpha_{0}}\left(\int\limits_{\Real^{n}} \varphi^{*}(x) L_{0}(x) \,dx - \int\limits_{\Real^{n}} \tilde{\varphi}(x) L_{0}(x) \,dx\right) \geqslant 0
    \end{multline*}
    Последнее верно, т.к. $\alpha(\tilde{\varphi}) \leqslant \alpha_0 = \alpha(\varphi^*)$.
    
    \smallskip
    Следовательно, $W \left(\theta_1, \varphi^{*}\right) - W(\theta_1, \tilde{\varphi}) \geqslant C_{\alpha_{0}}\bigl(\alpha\left(\varphi^{*}\right)-\alpha(\tilde{\varphi})\bigr)$, откуда получаем неравенство:

    \begin{equation*}
        W\left(\theta_1, \varphi^{*}\right) \geqslant W(\theta_1, \tilde{\varphi})
    \end{equation*}
    Что и требовалось доказать.
    \end{enumerate}
\end{proof}

\begin{rmrk}
    Все вышесказанное относится к случаю, когда мы проверяем две простые гипотезы: основную $H_0\colon \theta = \theta_0$ и альтернативу $H_1\colon \theta = \theta_1$.
    В случае сложных гипотез следует зафиксировать некоторые $\theta_0 \in \Theta_0, \: \theta_1 \in \Theta_1$ и применить лемму Неймана"--~Пирсона уже для простых гипотез.
    Если в результате построенный критерий не зависит от значения $\theta_1$, то он называется \textit{равномерно наиболее мощным критерием} (сокращённо РНМК).
\end{rmrk}
